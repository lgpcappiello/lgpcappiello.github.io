<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 ANOVA | Introduction to Statistics</title>
  <meta name="description" content="Chapter 10 ANOVA | Introduction to Statistics." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 ANOVA | Introduction to Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Chapter 10 ANOVA | Introduction to Statistics." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 ANOVA | Introduction to Statistics" />
  
  <meta name="twitter:description" content="Chapter 10 ANOVA | Introduction to Statistics." />
  

<meta name="author" content="Dr. Lauren Cappiello" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chi-square-tests.html"/>
<link rel="next" href="regression-and-correlation.html"/>
<script src="libs/header-attrs-2.20.1/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Statistics</a></li>

<li class="divider"></li>
<li><a href="index.html#welcome-to-statistics" id="toc-welcome-to-statistics">Welcome to Statistics!</a>
<ul>
<li><a href="index.html#course-learning-outcomes" id="toc-course-learning-outcomes">Course Learning Outcomes</a></li>
<li><a href="index.html#for-the-instructor" id="toc-for-the-instructor">For the Instructor</a></li>
<li><a href="index.html#for-the-student" id="toc-for-the-student">For the Student</a>
<ul>
<li><a href="index.html#r-programming" id="toc-r-programming">R Programming</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-data.html#introduction-to-data" id="toc-introduction-to-data"><span class="toc-section-number">1</span> Introduction to Data</a>
<ul>
<li><a href="introduction-to-data.html#chapter-overview" id="toc-chapter-overview"><span class="toc-section-number">1.1</span> Chapter Overview</a></li>
<li><a href="introduction-to-data.html#statistics-terminology" id="toc-statistics-terminology"><span class="toc-section-number">1.2</span> Statistics Terminology</a>
<ul>
<li><a href="introduction-to-data.html#r-entering-data" id="toc-r-entering-data">R: Entering Data</a></li>
<li><a href="introduction-to-data.html#section-exercises" id="toc-section-exercises">Section Exercises</a></li>
</ul></li>
<li><a href="introduction-to-data.html#sampling-and-design" id="toc-sampling-and-design"><span class="toc-section-number">1.3</span> Sampling and Design</a>
<ul>
<li><a href="introduction-to-data.html#statistical-sampling" id="toc-statistical-sampling"><span class="toc-section-number">1.3.1</span> Statistical Sampling</a></li>
<li><a href="introduction-to-data.html#experimental-design" id="toc-experimental-design"><span class="toc-section-number">1.3.2</span> Experimental Design</a></li>
<li><a href="introduction-to-data.html#r-random-number-generation" id="toc-r-random-number-generation">R: Random Number Generation</a></li>
<li><a href="introduction-to-data.html#section-exercises-1" id="toc-section-exercises-1">Section Exercises</a></li>
</ul></li>
<li><a href="introduction-to-data.html#frequency-distributions" id="toc-frequency-distributions"><span class="toc-section-number">1.4</span> Frequency Distributions</a>
<ul>
<li><a href="introduction-to-data.html#qualitative-variables" id="toc-qualitative-variables"><span class="toc-section-number">1.4.1</span> Qualitative Variables</a></li>
<li><a href="introduction-to-data.html#quantitative-variables" id="toc-quantitative-variables"><span class="toc-section-number">1.4.2</span> Quantitative Variables</a></li>
<li><a href="introduction-to-data.html#r-histograms" id="toc-r-histograms">R: Histograms</a></li>
</ul></li>
</ul></li>
<li><a href="descriptive-measures.html#descriptive-measures" id="toc-descriptive-measures"><span class="toc-section-number">2</span> Descriptive Measures</a>
<ul>
<li><a href="descriptive-measures.html#chapter-overview-1" id="toc-chapter-overview-1"><span class="toc-section-number">2.1</span> Chapter Overview</a></li>
<li><a href="descriptive-measures.html#measures-of-central-tendency" id="toc-measures-of-central-tendency"><span class="toc-section-number">2.2</span> Measures of Central Tendency</a>
<ul>
<li><a href="descriptive-measures.html#r-finding-measures-of-center" id="toc-r-finding-measures-of-center">R: Finding Measures of Center</a></li>
</ul></li>
<li><a href="descriptive-measures.html#measures-of-variability" id="toc-measures-of-variability"><span class="toc-section-number">2.3</span> Measures of Variability</a>
<ul>
<li><a href="descriptive-measures.html#r-finding-measures-of-variability" id="toc-r-finding-measures-of-variability">R: Finding Measures of Variability</a></li>
</ul></li>
<li><a href="descriptive-measures.html#measures-of-position" id="toc-measures-of-position"><span class="toc-section-number">2.4</span> Measures of Position</a>
<ul>
<li><a href="descriptive-measures.html#box-plots" id="toc-box-plots"><span class="toc-section-number">2.4.1</span> Box Plots</a></li>
<li><a href="descriptive-measures.html#r-measures-of-position" id="toc-r-measures-of-position">R: Measures of Position</a></li>
<li><a href="descriptive-measures.html#r-box-plots" id="toc-r-box-plots">R: Box Plots</a></li>
</ul></li>
<li><a href="descriptive-measures.html#descriptive-measures-for-populations" id="toc-descriptive-measures-for-populations"><span class="toc-section-number">2.5</span> Descriptive Measures for Populations</a></li>
</ul></li>
<li><a href="probability-concepts.html#probability-concepts" id="toc-probability-concepts"><span class="toc-section-number">3</span> Probability Concepts</a>
<ul>
<li><a href="probability-concepts.html#chapter-overview-2" id="toc-chapter-overview-2"><span class="toc-section-number">3.1</span> Chapter Overview</a></li>
<li><a href="probability-concepts.html#experiments-sample-spaces-and-events" id="toc-experiments-sample-spaces-and-events"><span class="toc-section-number">3.2</span> Experiments, Sample Spaces, and Events</a></li>
<li><a href="probability-concepts.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">3.3</span> Probability Distributions</a>
<ul>
<li><a href="probability-concepts.html#venn-diagrams" id="toc-venn-diagrams"><span class="toc-section-number">3.3.1</span> Venn Diagrams</a></li>
<li><a href="probability-concepts.html#probability-axioms" id="toc-probability-axioms"><span class="toc-section-number">3.3.2</span> Probability Axioms</a></li>
<li><a href="probability-concepts.html#exercises" id="toc-exercises">Exercises</a></li>
</ul></li>
<li><a href="probability-concepts.html#rules-of-probability" id="toc-rules-of-probability"><span class="toc-section-number">3.4</span> Rules of Probability</a>
<ul>
<li><a href="probability-concepts.html#addition-rules" id="toc-addition-rules"><span class="toc-section-number">3.4.1</span> Addition Rules</a></li>
<li><a href="probability-concepts.html#complements" id="toc-complements"><span class="toc-section-number">3.4.2</span> Complements</a></li>
</ul></li>
<li><a href="probability-concepts.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">3.5</span> Conditional Probability</a>
<ul>
<li><a href="probability-concepts.html#multiplication-rules" id="toc-multiplication-rules"><span class="toc-section-number">3.5.1</span> Multiplication Rules</a></li>
</ul></li>
</ul></li>
<li><a href="random-variables.html#random-variables" id="toc-random-variables"><span class="toc-section-number">4</span> Random Variables</a>
<ul>
<li><a href="random-variables.html#chapter-overview-3" id="toc-chapter-overview-3"><span class="toc-section-number">4.1</span> Chapter Overview</a></li>
<li><a href="random-variables.html#discrete-random-variables" id="toc-discrete-random-variables"><span class="toc-section-number">4.2</span> Discrete Random Variables</a>
<ul>
<li><a href="random-variables.html#the-mean-and-standard-deviation" id="toc-the-mean-and-standard-deviation"><span class="toc-section-number">4.2.1</span> The Mean and Standard Deviation</a></li>
</ul></li>
<li><a href="random-variables.html#the-binomial-distribution" id="toc-the-binomial-distribution"><span class="toc-section-number">4.3</span> The Binomial Distribution</a>
<ul>
<li><a href="random-variables.html#mean-and-variance" id="toc-mean-and-variance"><span class="toc-section-number">4.3.1</span> Mean and Variance</a></li>
<li><a href="random-variables.html#binomial-probabilities-in-r" id="toc-binomial-probabilities-in-r">Binomial Probabilities in R</a></li>
</ul></li>
<li><a href="random-variables.html#the-normal-distribution" id="toc-the-normal-distribution"><span class="toc-section-number">4.4</span> The Normal Distribution</a>
<ul>
<li><a href="random-variables.html#the-standard-normal-distribution" id="toc-the-standard-normal-distribution"><span class="toc-section-number">4.4.1</span> The Standard Normal Distribution</a></li>
</ul></li>
<li><a href="random-variables.html#area-under-the-standard-normal-curve" id="toc-area-under-the-standard-normal-curve"><span class="toc-section-number">4.5</span> Area Under the Standard Normal Curve</a>
<ul>
<li><a href="random-variables.html#r-normal-distribution-probabilities" id="toc-r-normal-distribution-probabilities">R: Normal Distribution Probabilities</a></li>
</ul></li>
<li><a href="random-variables.html#working-with-normally-distributed-variables" id="toc-working-with-normally-distributed-variables"><span class="toc-section-number">4.6</span> Working with Normally Distributed Variables</a>
<ul>
<li><a href="random-variables.html#normal-distribution-probabilities" id="toc-normal-distribution-probabilities"><span class="toc-section-number">4.6.1</span> Normal Distribution Probabilities</a></li>
<li><a href="random-variables.html#empirical-rule-for-variables" id="toc-empirical-rule-for-variables"><span class="toc-section-number">4.6.2</span> Empirical Rule for Variables</a></li>
<li><a href="random-variables.html#percentiles" id="toc-percentiles"><span class="toc-section-number">4.6.3</span> Percentiles</a></li>
</ul></li>
</ul></li>
<li><a href="introduction-to-confidence-intervals.html#introduction-to-confidence-intervals" id="toc-introduction-to-confidence-intervals"><span class="toc-section-number">5</span> Introduction to Confidence Intervals</a>
<ul>
<li><a href="introduction-to-confidence-intervals.html#chapter-overview-4" id="toc-chapter-overview-4"><span class="toc-section-number">5.1</span> Chapter Overview</a></li>
<li><a href="introduction-to-confidence-intervals.html#sampling-distributions" id="toc-sampling-distributions"><span class="toc-section-number">5.2</span> Sampling Distributions</a>
<ul>
<li><a href="introduction-to-confidence-intervals.html#sampling-error" id="toc-sampling-error"><span class="toc-section-number">5.2.1</span> Sampling Error</a></li>
<li><a href="introduction-to-confidence-intervals.html#the-central-limit-theorem" id="toc-the-central-limit-theorem"><span class="toc-section-number">5.2.2</span> The Central Limit Theorem</a></li>
</ul></li>
<li><a href="introduction-to-confidence-intervals.html#developing-confidence-intervals" id="toc-developing-confidence-intervals"><span class="toc-section-number">5.3</span> Developing Confidence Intervals</a>
<ul>
<li><a href="introduction-to-confidence-intervals.html#interpreting-a-confidence-interval" id="toc-interpreting-a-confidence-interval"><span class="toc-section-number">5.3.1</span> Interpreting a Confidence Interval</a></li>
<li><a href="introduction-to-confidence-intervals.html#exercises-1" id="toc-exercises-1"><span class="toc-section-number">5.3.2</span> Exercises</a></li>
</ul></li>
<li><a href="introduction-to-confidence-intervals.html#other-levels-of-confidence" id="toc-other-levels-of-confidence"><span class="toc-section-number">5.4</span> Other Levels of Confidence</a>
<ul>
<li><a href="introduction-to-confidence-intervals.html#r-finding-critical-values" id="toc-r-finding-critical-values">R: Finding Critical Values</a></li>
<li><a href="introduction-to-confidence-intervals.html#breaking-down-a-confidence-interval" id="toc-breaking-down-a-confidence-interval"><span class="toc-section-number">5.4.1</span> Breaking Down a Confidence Interval</a></li>
<li><a href="introduction-to-confidence-intervals.html#confidence-level-precision-and-sample-size" id="toc-confidence-level-precision-and-sample-size"><span class="toc-section-number">5.4.2</span> Confidence Level, Precision, and Sample Size</a></li>
<li><a href="introduction-to-confidence-intervals.html#exercises-2" id="toc-exercises-2"><span class="toc-section-number">5.4.3</span> Exercises</a></li>
</ul></li>
<li><a href="introduction-to-confidence-intervals.html#confidence-intervals-for-a-mean" id="toc-confidence-intervals-for-a-mean"><span class="toc-section-number">5.5</span> Confidence Intervals for a Mean</a>
<ul>
<li><a href="introduction-to-confidence-intervals.html#the-t-distribution" id="toc-the-t-distribution"><span class="toc-section-number">5.5.1</span> The T-Distribution</a></li>
<li><a href="introduction-to-confidence-intervals.html#r-t-critical-values" id="toc-r-t-critical-values">R: T Critical Values</a></li>
</ul></li>
<li><a href="introduction-to-confidence-intervals.html#r-confidence-intevals-for-a-mean" id="toc-r-confidence-intevals-for-a-mean">R: Confidence Intevals for a Mean</a></li>
</ul></li>
<li><a href="introduction-to-hypothesis-testing.html#introduction-to-hypothesis-testing" id="toc-introduction-to-hypothesis-testing"><span class="toc-section-number">6</span> Introduction to Hypothesis Testing</a>
<ul>
<li><a href="introduction-to-hypothesis-testing.html#chapter-overview-5" id="toc-chapter-overview-5"><span class="toc-section-number">6.1</span> Chapter Overview</a></li>
<li><a href="introduction-to-hypothesis-testing.html#logic-of-hypothesis-testing" id="toc-logic-of-hypothesis-testing"><span class="toc-section-number">6.2</span> Logic of Hypothesis Testing</a>
<ul>
<li><a href="introduction-to-hypothesis-testing.html#decision-errors" id="toc-decision-errors"><span class="toc-section-number">6.2.1</span> Decision Errors</a></li>
</ul></li>
<li><a href="introduction-to-hypothesis-testing.html#confidence-interval-approach-to-hypothesis-testing" id="toc-confidence-interval-approach-to-hypothesis-testing"><span class="toc-section-number">6.3</span> Confidence Interval Approach to Hypothesis Testing</a></li>
<li><a href="introduction-to-hypothesis-testing.html#critical-value-approach-to-hypothesis-testing" id="toc-critical-value-approach-to-hypothesis-testing"><span class="toc-section-number">6.4</span> Critical Value Approach to Hypothesis Testing</a>
<ul>
<li><a href="introduction-to-hypothesis-testing.html#test-statistics" id="toc-test-statistics"><span class="toc-section-number">6.4.1</span> Test statistics</a></li>
</ul></li>
<li><a href="introduction-to-hypothesis-testing.html#p-value-approach-to-hypothesis-testing" id="toc-p-value-approach-to-hypothesis-testing"><span class="toc-section-number">6.5</span> P-Value Approach to Hypothesis Testing</a>
<ul>
<li><a href="introduction-to-hypothesis-testing.html#p-values" id="toc-p-values"><span class="toc-section-number">6.5.1</span> P-Values</a></li>
</ul></li>
<li><a href="introduction-to-hypothesis-testing.html#r-hypothesis-tests-for-a-mean" id="toc-r-hypothesis-tests-for-a-mean">R: Hypothesis Tests for a Mean</a></li>
</ul></li>
<li><a href="inference-for-a-proportion.html#inference-for-a-proportion" id="toc-inference-for-a-proportion"><span class="toc-section-number">7</span> Inference for a Proportion</a>
<ul>
<li><a href="inference-for-a-proportion.html#chapter-overview-6" id="toc-chapter-overview-6"><span class="toc-section-number">7.1</span> Chapter Overview</a></li>
<li><a href="inference-for-a-proportion.html#confidence-intervals-for-a-proportion" id="toc-confidence-intervals-for-a-proportion"><span class="toc-section-number">7.2</span> Confidence Intervals for a Proportion</a></li>
<li><a href="inference-for-a-proportion.html#hypothesis-tests-for-a-proportion" id="toc-hypothesis-tests-for-a-proportion"><span class="toc-section-number">7.3</span> Hypothesis Tests for a Proportion</a>
<ul>
<li><a href="inference-for-a-proportion.html#confidence-interval-approach" id="toc-confidence-interval-approach"><span class="toc-section-number">7.3.1</span> Confidence Interval Approach</a></li>
<li><a href="inference-for-a-proportion.html#critical-value-approach" id="toc-critical-value-approach"><span class="toc-section-number">7.3.2</span> Critical Value Approach</a></li>
<li><a href="inference-for-a-proportion.html#p-value-approach" id="toc-p-value-approach"><span class="toc-section-number">7.3.3</span> P-Value Approach</a></li>
</ul></li>
<li><a href="inference-for-a-proportion.html#r-hypothesis-tests-for-a-proportion" id="toc-r-hypothesis-tests-for-a-proportion">R: Hypothesis Tests for a Proportion</a></li>
</ul></li>
<li><a href="inference-comparing-parameters.html#inference-comparing-parameters" id="toc-inference-comparing-parameters"><span class="toc-section-number">8</span> Inference: Comparing Parameters</a>
<ul>
<li><a href="inference-comparing-parameters.html#chapter-overview-7" id="toc-chapter-overview-7"><span class="toc-section-number">8.1</span> Chapter Overview</a></li>
<li><a href="inference-comparing-parameters.html#hypothesis-tests-for-two-proportions" id="toc-hypothesis-tests-for-two-proportions"><span class="toc-section-number">8.2</span> Hypothesis Tests for Two Proportions</a>
<ul>
<li><a href="inference-comparing-parameters.html#confidence-intervals-for-two-proportions" id="toc-confidence-intervals-for-two-proportions"><span class="toc-section-number">8.2.1</span> Confidence Intervals for Two Proportions</a></li>
<li><a href="inference-comparing-parameters.html#critical-values-test-statistics-and-p-values" id="toc-critical-values-test-statistics-and-p-values"><span class="toc-section-number">8.2.2</span> Critical Values, Test Statistics, and P-Values</a></li>
<li><a href="inference-comparing-parameters.html#r-hypothesis-tests-for-two-proportions" id="toc-r-hypothesis-tests-for-two-proportions">R: Hypothesis Tests for Two Proportions</a></li>
</ul></li>
<li><a href="inference-comparing-parameters.html#hypothesis-tests-for-two-means" id="toc-hypothesis-tests-for-two-means"><span class="toc-section-number">8.3</span> Hypothesis Tests for Two Means</a>
<ul>
<li><a href="inference-comparing-parameters.html#paired-samples" id="toc-paired-samples"><span class="toc-section-number">8.3.1</span> Paired Samples</a></li>
<li><a href="inference-comparing-parameters.html#independent-samples" id="toc-independent-samples"><span class="toc-section-number">8.3.2</span> Independent Samples</a></li>
<li><a href="inference-comparing-parameters.html#r-hypothesis-tests-for-two-means" id="toc-r-hypothesis-tests-for-two-means">R: Hypothesis Tests for Two Means</a></li>
</ul></li>
</ul></li>
<li><a href="chi-square-tests.html#chi-square-tests" id="toc-chi-square-tests"><span class="toc-section-number">9</span> Chi-Square Tests</a>
<ul>
<li><a href="chi-square-tests.html#chapter-overview-8" id="toc-chapter-overview-8"><span class="toc-section-number">9.1</span> Chapter Overview</a></li>
<li><a href="chi-square-tests.html#inference-for-a-population-variance" id="toc-inference-for-a-population-variance"><span class="toc-section-number">9.2</span> Inference for a Population Variance</a>
<ul>
<li><a href="chi-square-tests.html#the-chi-square-distribution" id="toc-the-chi-square-distribution"><span class="toc-section-number">9.2.1</span> The Chi-Square Distribution</a></li>
</ul></li>
<li><a href="chi-square-tests.html#the-ratio-of-two-variances" id="toc-the-ratio-of-two-variances"><span class="toc-section-number">9.3</span> The Ratio of Two Variances</a></li>
<li><a href="chi-square-tests.html#goodness-of-fit" id="toc-goodness-of-fit"><span class="toc-section-number">9.4</span> Goodness of Fit</a></li>
<li><a href="chi-square-tests.html#contingency-tables" id="toc-contingency-tables"><span class="toc-section-number">9.5</span> Contingency Tables</a></li>
</ul></li>
<li><a href="anova.html#anova" id="toc-anova"><span class="toc-section-number">10</span> ANOVA</a>
<ul>
<li><a href="anova.html#chapter-overview-9" id="toc-chapter-overview-9"><span class="toc-section-number">10.1</span> Chapter Overview</a></li>
<li><a href="anova.html#what-is-the-analysis-of-variance-anova" id="toc-what-is-the-analysis-of-variance-anova"><span class="toc-section-number">10.2</span> What is the Analysis of Variance (ANOVA)</a></li>
<li><a href="anova.html#the-f-distribution" id="toc-the-f-distribution"><span class="toc-section-number">10.3</span> The F-Distribution</a></li>
<li><a href="anova.html#multiple-comparisons-and-type-i-error-rate" id="toc-multiple-comparisons-and-type-i-error-rate"><span class="toc-section-number">10.4</span> Multiple Comparisons and Type I Error Rate</a></li>
</ul></li>
<li><a href="regression-and-correlation.html#regression-and-correlation" id="toc-regression-and-correlation"><span class="toc-section-number">11</span> Regression and Correlation</a>
<ul>
<li><a href="regression-and-correlation.html#chapter-overview-10" id="toc-chapter-overview-10"><span class="toc-section-number">11.1</span> Chapter Overview</a></li>
<li><a href="regression-and-correlation.html#linear-equations" id="toc-linear-equations"><span class="toc-section-number">11.2</span> Linear Equations</a></li>
<li><a href="regression-and-correlation.html#correlation" id="toc-correlation"><span class="toc-section-number">11.3</span> Correlation</a></li>
<li><a href="regression-and-correlation.html#finding-a-regression-line" id="toc-finding-a-regression-line"><span class="toc-section-number">11.4</span> Finding a Regression Line</a>
<ul>
<li><a href="regression-and-correlation.html#coefficient-of-determination" id="toc-coefficient-of-determination"><span class="toc-section-number">11.4.1</span> Coefficient of Determination</a></li>
</ul></li>
</ul></li>
<li><a href="appendices.html#appendices" id="toc-appendices">Appendices</a>
<ul>
<li><a href="appendices.html#appendix-a-important-links-and-additional-resources" id="toc-appendix-a-important-links-and-additional-resources">Appendix A: Important Links and Additional Resources</a>
<ul>
<li><a href="appendices.html#applets" id="toc-applets">Applets</a></li>
<li><a href="appendices.html#run-r-online" id="toc-run-r-online">Run R Online</a></li>
</ul></li>
<li><a href="appendices.html#appendix-b-average-deviance" id="toc-appendix-b-average-deviance">Appendix B: Average Deviance</a></li>
<li><a href="appendices.html#appendix-c-deriving-a-confidence-interval" id="toc-appendix-c-deriving-a-confidence-interval">Appendix C: Deriving a Confidence Interval</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="anova" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> ANOVA<a href="anova.html#anova" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="chapter-overview-9" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Chapter Overview<a href="anova.html#chapter-overview-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we extend the concepts from Chapter 6 to answer questions like “is there a difference between these means?” We will also consider hypothesis tests for whether a sample represents the population or closely matches a particular distribution.</p>
<p><strong>Chapter Learning Outcomes/Objectives</strong></p>
<p>Perform and interpret inference for</p>
<ol style="list-style-type: decimal">
<li>Interpret an ANOVA.</li>
<li>Use the Bonferroni correction to conduct multiple comparisons.</li>
</ol>
</div>
<div id="what-is-the-analysis-of-variance-anova" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> What is the Analysis of Variance (ANOVA)<a href="anova.html#what-is-the-analysis-of-variance-anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now that we’ve examined tests for one and two means, it’s natural to wonder about three or more means. For example, we might want to compare three different medications: treatment 1 (<span class="math inline">\(t_1\)</span>), treatment 2 (<span class="math inline">\(t_2\)</span>), and treatment 3 (<span class="math inline">\(t_3\)</span>). Based on what we’ve learned so far, we might think to do pairwise comparisons, examining <span class="math inline">\(t_1\)</span> vs <span class="math inline">\(t_2\)</span>, then <span class="math inline">\(t_2\)</span> vs <span class="math inline">\(t_3\)</span>, then <span class="math inline">\(t_1\)</span> vs <span class="math inline">\(t_3\)</span>. Unfortunately, this tends to increase our Type I error!</p>
<p>Think of it this way: if I set my confidence level to 95%, I’m setting my Type I error rate to <span class="math inline">\(\alpha=0.05\)</span>. In general terms, this means that about 1 out of every 20 times I run my experiment, I would make a type I error. If I went ahead and ran, say, 20 tests comparing two means, my <em>overall</em> Type I error rate is going to increase - there’s a pretty significant chance that at least one of those comparisons will results in a Type I error!</p>
<p>Instead, we will use a test that allows us to ask: “Are all these means the same?” This is called the <strong>an</strong>alysis <strong>o</strong>f <strong>va</strong>riance, or ANOVA.</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The mean outcome is the same across all groups.</li>
<li><span class="math inline">\(H_A\)</span>: At least one mean differs from the rest.</li>
</ul>
<p>In statistical notation, these hypotheses look like:</p>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \dots = \mu_k\)</span></li>
<li><span class="math inline">\(H_A: \mu_i \ne \mu_j\)</span> for at least one pair <span class="math inline">\((i, j)\)</span></li>
</ul>
<p>where <span class="math inline">\(k\)</span> is the number of means being compared and the notation <span class="math inline">\(\mu_i\)</span> represents the mean for the <span class="math inline">\(i\)</span>th group (<span class="math inline">\(i\)</span> can take on any whole number value between 1 and <span class="math inline">\(k\)</span>).</p>
<p>For ANOVA, we have three key conditions:</p>
<ol style="list-style-type: decimal">
<li>Observations are independent within and across groups.</li>
</ol>
<p>Independence within groups is the way we’ve been thinking about independence already. We want to convince ourselves that for any particular group, the observations do not impact each other. For independence across groups, we want to convince ourselves that the groups do not impact each other. Note: if we have a simple random sample, this assumption is always satisfied.</p>
<ol start="2" style="list-style-type: decimal">
<li>Data within each group are approximately normal.</li>
</ol>
<p>If you make a histogram of the data for each group, each histogram will look approximately bell-shaped.</p>
<ol start="3" style="list-style-type: decimal">
<li>Variability is approximately equal across groups.</li>
</ol>
<p>Take the standard deviation for each group and check if they are approximately equal. A boxplot is an appropriate way to do this visually.</p>
<center>
<font size='4'><strong>Why Variance?</strong>
</center>
<p></font></p>
<p>You may have seen the name “analysis of variance” and wondered what the variance has to do with comparing many means. Consider the following boxplots:</p>
<p><img src="IntroStats_files/figure-html/unnamed-chunk-86-1.png" width="672" /></p>
<p>Is there a difference in the means for Experiment 1? What about Experiment 2?</p>
<p>In fact, the means are <span class="math inline">\(\mu_1 = \mu_4 = 2\)</span>, <span class="math inline">\(\mu_2 = \mu_5 = 1\)</span>, and <span class="math inline">\(\mu_3 = \mu_6 = 0.5\)</span>. But the variances for the Experiment 1 groups are much larger than for the Experiment 2 groups! The larger variances in Experiment 1 obscure any differences between the group means. It is for this reason that we analyze variance as part of our test for differences in means.</p>
<blockquote>
<p>Aside: Why can’t we look at the data first and just test the two means that have the largest difference?</p>
<p>When we look at the data <em>and then choose a test</em>, this inflates our Type I error rate! It’s bad practice and not something we want to engage in as scientists.</p>
</blockquote>
<p>In order to perform an ANOVA, we need to consider whether the sample means differ more than we would expect them to based on natural variation (remember that we expect random samples to produce slightly different sample statistics each time!). This type of variation is called <strong>mean square between groups</strong> or <span class="math inline">\(MSG\)</span>. It has associated degrees of freedom <span class="math inline">\(df_G = k-1\)</span> where <span class="math inline">\(k\)</span> is the number of groups. Note that <span class="math display">\[MSG = \frac{SSG}{df_G}\]</span> where <span class="math inline">\(SSE\)</span> is the <strong>sum of squares group</strong>. If the null hypothesis is true, variation in the sample means is due to chance. In this case, we would expect the MSG to be relatively small.</p>
<p>When I say “relatively small”, I mean we need to compare this quantity to something. We need some quantity that will give us an idea of how much variability to expect if the null hypothesis is true. This is the <strong>mean square error</strong> or <span class="math inline">\(MSE\)</span>, which has degrees of freedom <span class="math inline">\(df_E = n-k\)</span>. Again, we have the relationship that <span class="math display">\[MSE = \frac{SSE}{df_E}\]</span> where <span class="math inline">\(SSE\)</span> is the <strong>sum of squares error</strong>. These calculations are very similar to the calculation for variance (and standard deviation)! (Note: we will not calculate these quantities by hand, but if you are interested in the mathematical details they are available in the OpenIntro Statistics textbook in the footnote on page 289.)</p>
<p>We compare these two quantities by examining their ratio: <span class="math display">\[F = \frac{MSG}{MSE}\]</span> This is the test statistic for the ANOVA.</p>
</div>
<div id="the-f-distribution" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> The F-Distribution<a href="anova.html#the-f-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <span class="math inline">\(\boldsymbol{F}\)</span><strong>-test</strong> relies on something called the <span class="math inline">\(F\)</span> distribution. The <span class="math inline">\(F\)</span> distribution has two parameters: <span class="math inline">\(df_1=df_G\)</span> and <span class="math inline">\(df_1=df_E\)</span>. The <span class="math inline">\(F\)</span> distribution always takes on positive values, so an <em>extreme</em> or <em>unusual</em> value for the <span class="math inline">\(F\)</span> distribution will correspond to a large (positive) number.</p>
<p>When we run an ANOVA, we almost always use the p-value approach. If you are using <code>R</code> for your distributions, the command is <code>pf(F, df1, df2, lower.tail=FALSE)</code> where <code>F</code> is the test statistic.</p>
<blockquote>
<p><em>Example:</em> Suppose I have a test with 100 observations and 5 groups. I find <span class="math inline">\(MSG = 0.041\)</span> and <span class="math inline">\(MSE = 0.023\)</span>. Then <span class="math display">\[df_G = k-1 = 5-1 = 4\]</span> and <span class="math display">\[df_E = n-k = 100-5 = 95\]</span> The test statistic is <span class="math display">\[f = \frac{0.041}{0.023} = 1.7826\]</span> To find the p-value using <code>R</code>, I would write the command</p>
</blockquote>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="anova.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pf</span>(<span class="fl">1.7826</span>, <span class="dv">4</span>, <span class="dv">95</span>, <span class="at">lower.tail=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.1387132</code></pre>
<blockquote>
<p>and find a p-value of 0.1387.</p>
</blockquote>
<p><a href="https://homepage.divms.uiowa.edu/~mbognar/applets/f.html" target="blank">Here is a nice F-distribution applet</a>. For this applet, <span class="math inline">\(\nu_1 = df_1\)</span> and <span class="math inline">\(\nu_2 = df_2\)</span>. Plug in your <span class="math inline">\(F\)</span> test statistic where it indicates “x =” and your p=value will appear in the red box next to “P(X&gt;x)”. When you enter your degrees of freedom, a visualization will appear similar to those in the Rossman and Chance applets we used previously.</p>
<center>
<font size='4'><strong>The ANOVA Table</strong>
</center>
<p></font></p>
<p>Generally, when we run an ANOVA, we create an ANOVA table (or we have software create one for us!). This table looks something like this</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>df</th>
<th>Sum of Squares</th>
<th>Mean Squares</th>
<th>F Value</th>
<th>P-Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>group</td>
<td><span class="math inline">\(df_G\)</span></td>
<td><span class="math inline">\(SSG\)</span></td>
<td><span class="math inline">\(MSG\)</span></td>
<td><span class="math inline">\(F\)</span></td>
<td>p-value</td>
</tr>
<tr class="even">
<td>error</td>
<td><span class="math inline">\(df_E\)</span></td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(MSE\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Example:</em> chick weights</p>
<p><code>R</code> has data on the weights of chicks fed six different feeds (diets). Assume these data are based on a random sample of chicks. There are <span class="math inline">\(n=71\)</span> total observations and <span class="math inline">\(k=6\)</span> different feeds. Let’s assume we want to test with a 0.05 level of significance.</p>
<p>The ANOVA hypotheses are</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: the mean weight is the same for all six feeds.</li>
<li><span class="math inline">\(H_A\)</span>: at least one feed has a mean weight that differs.</li>
</ul>
<p>The summaries for these data are</p>
</blockquote>
<pre><code>##         casein horsebean linseed meatmeal soybean sunflower
## n        12.00     10.00   12.00    11.00   14.00     12.00
## Mean    323.58    160.20  218.75   276.91  246.43    328.92
## Std Dev  64.43     38.63   52.24    64.90   54.13     48.84</code></pre>
<p><img src="IntroStats_files/figure-html/unnamed-chunk-88-1.png" width="672" /><img src="IntroStats_files/figure-html/unnamed-chunk-88-2.png" width="672" /></p>
<blockquote>
<p>The group sizes are relatively small, so it’s difficult to determine how far from normality these data are based on the histograms. We may also run into some issues with constant variance. However, for the sake of the example, let’s push ahead with the ANOVA! Since we usually use software to calculate ANOVAs, I’ve used <code>R</code> to create the following ANOVA table:</p>
</blockquote>
<pre><code>## Analysis of Variance Table
## 
## Response: chickwts$weight
##               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## chickwts$feed  5 231129   46226  15.365 5.936e-10 ***
## Residuals     65 195556    3009                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<blockquote>
<p>From the table, we can confirm that <span class="math inline">\(df_G = 6-1 = 5\)</span> and <span class="math inline">\(df_E = 71 - 6 = 65\)</span>. The F test statistic is <span class="math display">\[MSG/MSE = 46226 / 3009 = 15.365\]</span> Finally, the p-value is <span class="math inline">\(5.936\times10^{-10}\)</span>. Clearly <span class="math inline">\(5.936\times10^{-10} &lt; \alpha = 0.05\)</span>, so we will reject the null hypothesis and conclude that at least one of the feed groups has a mean weight that differs.</p>
</blockquote>
</div>
<div id="multiple-comparisons-and-type-i-error-rate" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Multiple Comparisons and Type I Error Rate<a href="anova.html#multiple-comparisons-and-type-i-error-rate" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s return for a moment to our ANOVA hypotheses:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The mean outcome is the same across all groups.</li>
<li><span class="math inline">\(H_A\)</span>: At least one mean differs from the rest.</li>
</ul>
<p>If we reject <span class="math inline">\(H_0\)</span> and conclude that “at least one mean differs from the rest”, how do we determine which mean(s) differ? <em>If</em> we reject <span class="math inline">\(H_0\)</span>, we will perform a series of two-sample t-tests. But wait! What about the Type I error? Isn’t this exactly what we decided we couldn’t do when we introduced ANOVA?</p>
<p>In order to avoid this increased Type I error rate, we run these <strong>mulitple comparisons</strong> with a modified significance level. There are several ways to do this, but the most common way is with the <strong>Bonferroni correction</strong>. Here, if we want to test at the <span class="math inline">\(100(1-\alpha)\)</span> level of significance, we run each of our pairwise comparisons with <span class="math display">\[\alpha^* = \alpha/K\]</span> where <span class="math inline">\(K\)</span> is the number of comparisons being considered. For <span class="math inline">\(k\)</span> groups, there are <span class="math display">\[K = \frac{k(k-1)}{2}\]</span> possible pairwise comparisons.</p>
<p>For these comparisons, we use a special pooled estimate of the standard deviation, <span class="math inline">\(s_{\text{pooled}}\)</span> in place of <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>: <span class="math display">\[\text{standard error} = \sqrt{\frac{s_{\text{pooled}}^2}{n_1} + \frac{s_{\text{pooled}}^2}{n_2}}\]</span> Other than changing <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\alpha^*\)</span> and the standard error to this new formula, the test is exactly the same as that discussed in the previous section. Note that <span class="math display">\[s_{\text{pooled}} = \sqrt{MSE}\]</span> and the degrees of freedom is <span class="math inline">\(df_E\)</span>.</p>
<blockquote>
<p><em>Example</em>: chick weights</p>
<p>Let’s extend our discussion on the chick weights to multiple comparisons. Since we were able to conclude that at least one feed has a weight that differs, we want to find out where the difference(s) lie!</p>
<p>We will test all possible pairwise comparisons. This will require <span class="math inline">\(K = \frac{6(6-1)}{2} = 15\)</span> tests. The pooled standard deviation is <span class="math inline">\(s_{pooled} = \sqrt{3009} \approx 54.85\)</span>. Let’s walk through the test of casein <span class="math inline">\((\bar{x}_1 = 323.58, n=12)\)</span> vs horsebean <span class="math inline">\((\bar{x}_2 = 160.20, n=10)\)</span>:</p>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2\)</span></li>
<li><span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span></li>
</ul>
<p>The estimated difference and standard error are <span class="math display">\[\bar{x}_1 - \bar{x}_2 = 323.58 - 160.20 = 163.38 \quad\quad SE = \sqrt{\frac{54.85^2}{11}+\frac{54.85^2}{9}} = 25.65\]</span> which results in a test statistic of <span class="math inline">\(t=6.37\)</span> and a p-value of <span class="math inline">\(1.11\times10^{-8}\)</span>. We then compare this to <span class="math inline">\(\alpha^* = 0.05/15 = 0.0033\)</span>. Since the p-value of <span class="math inline">\(1.11\times10^{-8} &lt; \alpha^* = 0.0033\)</span>, we reject the null hypothesis and conclude there is a significant difference in mean chick weight between the casein and horsebean feeds.</p>
<p>In order to complete the pairwise comparisons, we would then run the remaining 14 tests. I will leave this as an optional exercise for the particularly motivated student.</p>
</blockquote>
<p>Note: occasionally, we may reject <span class="math inline">\(H_0\)</span> in the ANOVA but may fail to find any statistically significant differences when performing multiple comparisons with the Bonferroni correction. This is ok! It just means we were unable to identify which specific groups differ.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chi-square-tests.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-and-correlation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IntroStats.pdf", "IntroStats.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
