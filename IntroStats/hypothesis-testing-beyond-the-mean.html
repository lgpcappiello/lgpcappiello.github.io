<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Module 7 Hypothesis Testing: Beyond the Mean | Course Notes for Introduction to Statistics</title>
  <meta name="description" content="Course notes for Introduction to Statistics (Stat 1). These notes are based loosely on OpenInto Statistics (Diez, Cetinkaya-Rundel, and Barr) and Introductory Statisics (Weiss)." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Module 7 Hypothesis Testing: Beyond the Mean | Course Notes for Introduction to Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course notes for Introduction to Statistics (Stat 1). These notes are based loosely on OpenInto Statistics (Diez, Cetinkaya-Rundel, and Barr) and Introductory Statisics (Weiss)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Module 7 Hypothesis Testing: Beyond the Mean | Course Notes for Introduction to Statistics" />
  
  <meta name="twitter:description" content="Course notes for Introduction to Statistics (Stat 1). These notes are based loosely on OpenInto Statistics (Diez, Cetinkaya-Rundel, and Barr) and Introductory Statisics (Weiss)." />
  

<meta name="author" content="Dr.Â Lauren Cappiello" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-hypothesis-testing.html"/>

<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Course Notes for Introduction to Statistics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to Data</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#module-overview"><i class="fa fa-check"></i><b>1.1</b> Module Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#statistics-terminology"><i class="fa fa-check"></i><b>1.2</b> Statistics Terminology</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dig-deeper"><i class="fa fa-check"></i>Dig Deeper</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#data-basics"><i class="fa fa-check"></i><b>1.3</b> Data Basics</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#sampling"><i class="fa fa-check"></i><b>1.4</b> Sampling</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#experimental-design"><i class="fa fa-check"></i><b>1.5</b> Experimental Design</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#frequency-distributions"><i class="fa fa-check"></i><b>1.6</b> Frequency Distributions</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#qualitative-variables"><i class="fa fa-check"></i><b>1.6.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#quantitative-variables"><i class="fa fa-check"></i><b>1.6.2</b> Quantitative Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive-measures.html"><a href="descriptive-measures.html"><i class="fa fa-check"></i><b>2</b> Descriptive Measures</a>
<ul>
<li class="chapter" data-level="2.1" data-path="descriptive-measures.html"><a href="descriptive-measures.html#module-overview-1"><i class="fa fa-check"></i><b>2.1</b> Module Overview</a></li>
<li class="chapter" data-level="2.2" data-path="descriptive-measures.html"><a href="descriptive-measures.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>2.2</b> Measures of Central Tendency</a></li>
<li class="chapter" data-level="2.3" data-path="descriptive-measures.html"><a href="descriptive-measures.html#measures-of-variability"><i class="fa fa-check"></i><b>2.3</b> Measures of Variability</a></li>
<li class="chapter" data-level="2.4" data-path="descriptive-measures.html"><a href="descriptive-measures.html#measures-of-position"><i class="fa fa-check"></i><b>2.4</b> Measures of Position</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="descriptive-measures.html"><a href="descriptive-measures.html#box-plots"><i class="fa fa-check"></i><b>2.4.1</b> Box Plots</a></li>
<li class="chapter" data-level="2.4.2" data-path="descriptive-measures.html"><a href="descriptive-measures.html#descriptive-measures-for-populations"><i class="fa fa-check"></i><b>2.4.2</b> Descriptive Measures for Populations</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="descriptive-measures.html"><a href="descriptive-measures.html#regression-and-correlation"><i class="fa fa-check"></i><b>2.5</b> Regression and Correlation</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="descriptive-measures.html"><a href="descriptive-measures.html#correlation"><i class="fa fa-check"></i><b>2.5.1</b> Correlation</a></li>
<li class="chapter" data-level="2.5.2" data-path="descriptive-measures.html"><a href="descriptive-measures.html#finding-a-regression-line"><i class="fa fa-check"></i><b>2.5.2</b> Finding a Regression Line</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability-concepts.html"><a href="probability-concepts.html"><i class="fa fa-check"></i><b>3</b> Probability Concepts</a>
<ul>
<li class="chapter" data-level="3.1" data-path="probability-concepts.html"><a href="probability-concepts.html#module-overview-2"><i class="fa fa-check"></i><b>3.1</b> Module Overview</a></li>
<li class="chapter" data-level="3.2" data-path="probability-concepts.html"><a href="probability-concepts.html#experiments-sample-spaces-and-events"><i class="fa fa-check"></i><b>3.2</b> Experiments, Sample Spaces, and Events</a></li>
<li class="chapter" data-level="3.3" data-path="probability-concepts.html"><a href="probability-concepts.html#probability-distributions"><i class="fa fa-check"></i><b>3.3</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="probability-concepts.html"><a href="probability-concepts.html#venn-diagrams"><i class="fa fa-check"></i><b>3.3.1</b> Venn Diagrams</a></li>
<li class="chapter" data-level="3.3.2" data-path="probability-concepts.html"><a href="probability-concepts.html#probability-axioms"><i class="fa fa-check"></i><b>3.3.2</b> Probability Axioms</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="probability-concepts.html"><a href="probability-concepts.html#rules-of-probability"><i class="fa fa-check"></i><b>3.4</b> Rules of Probability</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="probability-concepts.html"><a href="probability-concepts.html#addition-rules"><i class="fa fa-check"></i><b>3.4.1</b> Addition Rules</a></li>
<li class="chapter" data-level="3.4.2" data-path="probability-concepts.html"><a href="probability-concepts.html#complements"><i class="fa fa-check"></i><b>3.4.2</b> Complements</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="probability-concepts.html"><a href="probability-concepts.html#conditional-probability"><i class="fa fa-check"></i><b>3.5</b> Conditional Probability</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="probability-concepts.html"><a href="probability-concepts.html#multiplication-rules"><i class="fa fa-check"></i><b>3.5.1</b> Multiplication Rules</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="random-variables.html"><a href="random-variables.html"><i class="fa fa-check"></i><b>4</b> Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="random-variables.html"><a href="random-variables.html#module-overview-3"><i class="fa fa-check"></i><b>4.1</b> Module Overview</a></li>
<li class="chapter" data-level="4.2" data-path="random-variables.html"><a href="random-variables.html#discrete-random-variables"><i class="fa fa-check"></i><b>4.2</b> Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="random-variables.html"><a href="random-variables.html#the-mean-and-standard-deviation"><i class="fa fa-check"></i><b>4.2.1</b> The Mean and Standard Deviation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="random-variables.html"><a href="random-variables.html#the-binomial-distribution"><i class="fa fa-check"></i><b>4.3</b> The Binomial Distribution</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="random-variables.html"><a href="random-variables.html#mean-and-variance"><i class="fa fa-check"></i><b>4.3.1</b> Mean and Variance</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="random-variables.html"><a href="random-variables.html#the-normal-distribution"><i class="fa fa-check"></i><b>4.4</b> The Normal Distribution</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="random-variables.html"><a href="random-variables.html#the-standard-normal-distribution"><i class="fa fa-check"></i><b>4.4.1</b> The Standard Normal Distribution</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="random-variables.html"><a href="random-variables.html#area-under-the-standard-normal-curve"><i class="fa fa-check"></i><b>4.5</b> Area Under the Standard Normal Curve</a></li>
<li class="chapter" data-level="4.6" data-path="random-variables.html"><a href="random-variables.html#working-with-normally-distributed-variables"><i class="fa fa-check"></i><b>4.6</b> Working with Normally Distributed Variables</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="random-variables.html"><a href="random-variables.html#normal-distribution-probabilities"><i class="fa fa-check"></i><b>4.6.1</b> Normal Distribution Probabilities</a></li>
<li class="chapter" data-level="4.6.2" data-path="random-variables.html"><a href="random-variables.html#empirical-rule-for-variables"><i class="fa fa-check"></i><b>4.6.2</b> Empirical Rule for Variables</a></li>
<li class="chapter" data-level="4.6.3" data-path="random-variables.html"><a href="random-variables.html#percentiles"><i class="fa fa-check"></i><b>4.6.3</b> Percentiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html"><i class="fa fa-check"></i><b>5</b> Introduction to Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#module-overview-4"><i class="fa fa-check"></i><b>5.1</b> Module Overview</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#sampling-distributions"><i class="fa fa-check"></i><b>5.2</b> Sampling Distributions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#sampling-error"><i class="fa fa-check"></i><b>5.2.1</b> Sampling Error</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#the-sampling-distribution-of-barx"><i class="fa fa-check"></i><b>5.2.2</b> The Sampling Distribution of <span class="math inline">\(\bar{X}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#developing-confidence-intervals"><i class="fa fa-check"></i><b>5.3</b> Developing Confidence Intervals</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#interpreting-a-confidence-interval"><i class="fa fa-check"></i><b>5.3.1</b> Interpreting a Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#other-levels-of-confidence"><i class="fa fa-check"></i><b>5.4</b> Other Levels of Confidence</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#confidence-level-precision-and-sample-size"><i class="fa fa-check"></i><b>5.4.1</b> Confidence Level, Precision, and Sample Size</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#confidence-intervals-sigma-unknown"><i class="fa fa-check"></i><b>5.5</b> Confidence Intervals, <span class="math inline">\(\sigma\)</span> Unknown</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#the-t-distribution"><i class="fa fa-check"></i><b>5.5.1</b> The T-Distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="introduction-to-inference.html"><a href="introduction-to-inference.html#summary-of-confidence-interval-settings"><i class="fa fa-check"></i><b>5.6</b> Summary of Confidence Interval Settings</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html"><i class="fa fa-check"></i><b>6</b> Introduction to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#module-overview-5"><i class="fa fa-check"></i><b>6.1</b> Module Overview</a></li>
<li class="chapter" data-level="6.2" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#logic-of-hypothesis-testing"><i class="fa fa-check"></i><b>6.2</b> Logic of Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#decision-errors"><i class="fa fa-check"></i><b>6.2.1</b> Decision Errors</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#confidence-interval-approach-to-hypothesis-testing"><i class="fa fa-check"></i><b>6.3</b> Confidence Interval Approach to Hypothesis Testing</a></li>
<li class="chapter" data-level="6.4" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#critical-value-approach-to-hypothesis-testing"><i class="fa fa-check"></i><b>6.4</b> Critical Value Approach to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#test-statistics"><i class="fa fa-check"></i><b>6.4.1</b> Test statistics</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#p-value-approach-to-hypothesis-testing"><i class="fa fa-check"></i><b>6.5</b> P-Value Approach to Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="introduction-to-hypothesis-testing.html"><a href="introduction-to-hypothesis-testing.html#p-values"><i class="fa fa-check"></i><b>6.5.1</b> P-Values</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html"><i class="fa fa-check"></i><b>7</b> Hypothesis Testing: Beyond the Mean</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#module-overview-6"><i class="fa fa-check"></i><b>7.1</b> Module Overview</a></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#hypothesis-tests-for-two-means"><i class="fa fa-check"></i><b>7.2</b> Hypothesis Tests for Two Means</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#paired-samples"><i class="fa fa-check"></i><b>7.2.1</b> Paired Samples</a></li>
<li class="chapter" data-level="7.2.2" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#independent-samples"><i class="fa fa-check"></i><b>7.2.2</b> Independent Samples</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#analysis-of-variance-anova"><i class="fa fa-check"></i><b>7.3</b> Analysis of Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#the-f-distribution"><i class="fa fa-check"></i><b>7.3.1</b> The F-Distribution</a></li>
<li class="chapter" data-level="7.3.2" data-path="hypothesis-testing-beyond-the-mean.html"><a href="hypothesis-testing-beyond-the-mean.html#multiple-comparisons-and-type-i-error-rate"><i class="fa fa-check"></i><b>7.3.2</b> Multiple Comparisons and Type I Error Rate</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Course Notes for Introduction to Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="hypothesis-testing-beyond-the-mean" class="section level1" number="7">
<h1><span class="header-section-number">Module 7</span> Hypothesis Testing: Beyond the Mean</h1>
<div id="module-overview-6" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Module Overview</h2>
<p>In this module, we extend the concepts from Module 6 to answer questions like âis there a difference between these means?â We will also consider hypothesis tests for whether a sample represents the population or closely matches a particular distribution.</p>
</div>
<div id="hypothesis-tests-for-two-means" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Hypothesis Tests for Two Means</h2>
<p>What if we wanted to <em>compare</em> two means? We begin by discussing paired samples. This will feel very familiar, since itâs essentially the same as hypothesis testing for a single mean. Then we will move on to independent samples, which will require a couple of adjustments.</p>
<div id="paired-samples" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Paired Samples</h3>
<p>Sometimes there is a special correspondence between two sets of observations. We say that two sets of observations are <strong>paired</strong> if each observation has a natural connection with exactly one observation in the other data set. Consider the following data from 30 students given a pre- and post-test on a course concept:</p>
<table>
<thead>
<tr class="header">
<th align="center">Student</th>
<th align="center">Pre-Test</th>
<th align="center">Post-Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">52</td>
<td align="center">70</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">71</td>
<td align="center">98</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">13</td>
<td align="center">65</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\dots\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="odd">
<td align="center">30</td>
<td align="center">48</td>
<td align="center">81</td>
</tr>
</tbody>
</table>
<p>The natural connection between âpre-testâ and âpost-testâ is the student who took each test! Often, paired data will involve similar measures taken on the <em>same item or individual</em>. We <em>pair</em> these data because we want to compare two means, but we also want to account for the pairing.</p>
<p>Why? Consider: If a student got a 13% on the pre-test, I would love to see them get a 60% on the post-test - thatâs a huge improvement! But if a student got an 82% on the pre-test, I would <em>not</em> like to see them get a 60% on the post-test. Pairing the data lets us account for this connection.</p>
<p>So what do we do with paired data? Fortunately, this part is easy! We start by taking the difference between the two sets of observations. In the pre- and post-test example, I will take the pre-test score and subtract the post-test score:</p>
<table>
<thead>
<tr class="header">
<th align="center">Student</th>
<th align="center">Pre-Test</th>
<th align="center">Post-Test</th>
<th align="center"><strong>Difference</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">52</td>
<td align="center">70</td>
<td align="center"><strong>18</strong></td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">71</td>
<td align="center">98</td>
<td align="center"><strong>27</strong></td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">13</td>
<td align="center">65</td>
<td align="center"><strong>52</strong></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\dots\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
<td align="center"><span class="math inline">\(\dots\)</span></td>
</tr>
<tr class="odd">
<td align="center">30</td>
<td align="center">48</td>
<td align="center">81</td>
<td align="center"><strong>33</strong></td>
</tr>
</tbody>
</table>
<p>Then, we do a test of a <em>single mean</em> on the differences where</p>
<ul>
<li><span class="math inline">\(H_0: \mu_{\text{d}} = 0\)</span></li>
<li><span class="math inline">\(H_A: \mu_{\text{d}} \ne 0\)</span></li>
</ul>
<p>Note that the subscript âdâ denotes âdifference.â We will use the exact same test(s) as in the previous sections:</p>
<ul>
<li><p><strong>Setting 1</strong>: <span class="math inline">\(\mu_{\text{d}}\)</span> is target parameter, the differences are approximately normal, <span class="math inline">\(\sigma_{\text{d}}\)</span> known <span class="math display">\[z = \frac{\bar{x}_{\text{d}}}{\sigma_{\text{d}}/\sqrt{n_{\text{d}}}}\]</span> and the p-value is <span class="math display">\[2P(Z &gt; |z|)\]</span> where <span class="math inline">\(z\)</span> is the test statistic.</p></li>
<li><p><strong>Setting 2</strong>: <span class="math inline">\(\mu_{\text{d}}\)</span> is target parameter, <span class="math inline">\(n_{\text{d}} \ge 30\)</span>, <span class="math inline">\(\sigma_{\text{d}}\)</span> unknown <span class="math display">\[z = \frac{\bar{x}_{\text{d}}}{s_{\text{d}}/\sqrt{n_{\text{d}}}}\]</span> and the p-value is <span class="math display">\[2P(Z &gt; |z|)\]</span> where <span class="math inline">\(z\)</span> is the test statistic.</p></li>
<li><p><strong>Setting 3</strong>: <span class="math inline">\(\mu_{\text{d}}\)</span> is target parameter, <span class="math inline">\(n_{\text{d}} &lt; 30\)</span>, <span class="math inline">\(\sigma_{\text{d}}\)</span> unknown <span class="math display">\[t = \frac{\bar{x}_{\text{d}}}{s_{\text{d}}/\sqrt{n_{\text{d}}}}\]</span> and the p-value is <span class="math display">\[2P(t_{df} &gt; |t|)\]</span> where <span class="math inline">\(t\)</span> is the test statistic.</p></li>
</ul>
<p>Here, <span class="math inline">\(n_{\text{d}}\)</span> is the number of pairs.</p>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>State the null and alternative hypotheses.</li>
<li>Determine the significance level <span class="math inline">\(\alpha\)</span>. Check assumptions (decide which setting to use).</li>
<li>Compute the value of the test statistic.</li>
<li>Determine the critical values or p-value.</li>
<li>For the <em>critical value approach</em>: If the test statistic is in the rejection region, reject the null hypothesis. For the <em>p-value approach</em>: If <span class="math inline">\(\text{p-value} &lt; \alpha\)</span>, reject the null hypothesis. Otherwise, do not reject.</li>
<li>Interpret results.</li>
</ol>
</div>
<div id="independent-samples" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Independent Samples</h3>
<p>In <strong>independent samples</strong>, the sample from one population does not impact the sample from the other population. In short, we take two <em>separate samples</em> and compare them.</p>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 \quad \rightarrow \quad H_0: \mu_1 - \mu_2 = 0\)</span></li>
<li><span class="math inline">\(H_A: \mu_1 \ne \mu_2 \quad \rightarrow \quad H_A: \mu_1 - \mu_2 \ne 0\)</span></li>
</ul>
<p>If we use <span class="math inline">\(\bar{x}\)</span> to estimate <span class="math inline">\(\mu\)</span>, intuitively we might use <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span> to estimate <span class="math inline">\(\mu_1 - \mu_2\)</span>. To do this, we need to know something about the sampling distribution of <span class="math inline">\(\bar{x}_1-\bar{x}_2\)</span>.</p>
<p>Consider: if <span class="math inline">\(X_1\)</span> is Normal(<span class="math inline">\(\mu_1\)</span>, <span class="math inline">\(\sigma_1\)</span>) and <span class="math inline">\(X_2\)</span> is Normal(<span class="math inline">\(\mu_2\)</span>,<span class="math inline">\(\sigma_2\)</span>) with <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span> are known, then for independent samples of size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>,</p>
<ul>
<li><span class="math inline">\(\bar{X}_1-\bar{X}_2\)</span> is Normal(<span class="math inline">\(\mu_{\bar{X}_1-\bar{X}_2}\)</span>, <span class="math inline">\(\sigma_{\bar{X}_1-\bar{X}_2}\)</span>).</li>
<li><span class="math inline">\(\mu_{\bar{X}_1-\bar{X}_2} = \mu_1 - \mu_2\)</span></li>
<li><span class="math inline">\(\sigma_{\bar{X}_1-\bar{X}_2} = \sigma_1 - \sigma_2\)</span></li>
</ul>
<p>so then <span class="math display">\[Z = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{\sigma_1/n_1 - \sigma_2/n_2}}\]</span> has a standard normal distribution. But, as we mentioned earlier, we rarely work in that setting where the population standard deviation is known. Instead, we will use <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span> to estimate <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>. For independent samples of size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>, <span class="math display">\[t = \frac{(\bar{X}_1 - \bar{X}_2) - (\mu_1 - \mu_2)}{\sqrt{s_1/n_1 - s_2/n_2}}\]</span> has a t-distribution with degrees of freedom <span class="math display">\[\Delta = \frac{[(s_1^2/n_1) + (s_2^2/n_2)]^2}{\frac{(s_1^2/n_1)^2}{n_1-1} + \frac{(s_2^2/n_2)^2}{n_2-1}}\]</span> rounded <em>down</em> to the nearest whole number.</p>
<center>
<font size='4'><strong>The Two-Sample T-Test</strong>
</center>
<p></font></p>
<p>Assumptions:</p>
<ul>
<li>Simple random samples.</li>
<li>Independent samples.</li>
<li>Normal populations or large (<span class="math inline">\(n \ge 30\)</span>) samples.</li>
</ul>
<p><strong>Steps for Critical Value Approach</strong>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H_0: \mu_1 - \mu_2 = 0\)</span> and <span class="math inline">\(H_A: \mu_1 - \mu_2 \ne 0\)</span></li>
<li>Check assumptions; select the significance level <span class="math inline">\(\alpha\)</span>.</li>
<li>Compute the test statistic <span class="math display">\[t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1/n_1 - s_2/n_2}}\]</span> Note that we assume under the null hypothesis that <span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>, which is why we replace this quantity with <span class="math inline">\(0\)</span> in the test statistic.</li>
<li>The critical value is <span class="math inline">\(\pm t_{df, \alpha/2}\)</span> with <span class="math inline">\(df = \Delta\)</span>.</li>
<li>If the test statistic falls in the rejection region, reject the null hypothesis.</li>
<li>Interpret in the context of the problem.</li>
</ol>
<p><strong>Steps for P-Value Approach</strong>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H_0: \mu_1 - \mu_2 = 0\)</span> and <span class="math inline">\(H_A: \mu_1 - \mu_2 \ne 0\)</span></li>
<li>Check assumptions; select the significance level <span class="math inline">\(\alpha\)</span>.</li>
<li>Compute the test statistic <span class="math display">\[t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s_1/n_1 - s_2/n_2}}\]</span> Note that we assume under the null hypothesis that <span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>, which is why we replace this quantity with <span class="math inline">\(0\)</span> in the test statistic.</li>
<li>The p-value is <span class="math inline">\(2P(t_{df} &gt; |t|)\)</span> with <span class="math inline">\(df = \Delta\)</span>.</li>
<li>If <span class="math inline">\(\text{p-value}&lt;\alpha\)</span>, reject the null hypothesis.</li>
<li>Interpret in the context of the problem.</li>
</ol>
<p>Notice that the only difference between the critical value and p-value approaches are steps 4 and 5.</p>
<blockquote>
<p><em>Example</em>: Researchers wanted to detemine whether a dymanic or static approach would impact the time needed to complete neurosurgeries. The experiment resulted in the following data from simple random samples of patients:</p>
<table>
<thead>
<tr class="header">
<th align="center">Dynamic</th>
<th align="center">Static</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\bar{x}_1 = 394.6\)</span></td>
<td align="center"><span class="math inline">\(\bar{x}_2 = 468.3\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(s_1 = 84.7\)</span></td>
<td align="center"><span class="math inline">\(s_2 = 38.2\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(n_1 = 14\)</span></td>
<td align="center"><span class="math inline">\(n_2 = 6\)</span></td>
</tr>
</tbody>
</table>
<p>Times are measured in minutes. Assume <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are reasonably normal.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> and <span class="math inline">\(H_A: \mu_1\ne\mu_2\)</span></li>
<li>Let <span class="math inline">\(\alpha=0.05\)</span> (this will be our default when a significance level is not given)
<ul>
<li>We are told these are simple random samples.</li>
<li>Thereâs no reason that time for a neurosurgery with the dynamic system would impact time for the static system (or vice versa), so itâs reasonable to assume these samples are independent.</li>
<li>We are told to assume that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are reasonably normal.</li>
</ul></li>
<li>The test statistic is <span class="math display">\[t = \frac{394.6-468.3}{84.7^2/14 + 38.2^2/6} = -2.681\]</span></li>
<li>Then <span class="math display">\[df = \Delta = \frac{(84.7^2/14) + (38.2^2/6)^2}{\frac{(84.7^2/14)^2}{14-1} + \frac{(38.2^2/6)^2}{6-1}} = 17\]</span> when rounded down. The critical value is <span class="math display">\[t_{17, 0.025} = 2.110\]</span> and the p-value is <span class="math display">\[2P(t_{17}&gt;|-2.681|)=2(0.0079)=0.0158\]</span></li>
<li>For the critical value approach,</li>
</ol>
</blockquote>
<p><img src="IntroStats_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<blockquote>
<p>Since the test statistic is in the rejection region, we reject the null hypothesis. For the p-value approach, since <span class="math inline">\(\text{p-value}=0.158 &lt; \alpha =0.05\)</span>, reject the null hypothesis.</p>
<ol start="6" style="list-style-type: decimal">
<li>At the 0.05 level of significance, the data provide sufficient evidence to conclude that the mean time for the dynamic system is less than the mean time for the static system.</li>
</ol>
</blockquote>
<p>We can also construct a <strong><span class="math inline">\((1-\alpha)100\%\)</span> confidence interval</strong> for the difference of the two population means: <span class="math display">\[(\bar{x}_1-\bar{x}_2) \pm t_{df, \alpha/2}\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}\]</span> which we interpret as we interpret other confidence intervals, including in our interpretation that we are now considering the *difference of two means**.</p>
</div>
</div>
<div id="analysis-of-variance-anova" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Analysis of Variance (ANOVA)</h2>
<p>Now that weâve examined tests for one and two means, itâs natural to wonder about three or more means. For example, we might want to compare three different medications: treatment 1 (<span class="math inline">\(t_1\)</span>), treatment 2 (<span class="math inline">\(t_2\)</span>), and treatment 3 (<span class="math inline">\(t_3\)</span>). Based on what weâve learned so far, we might think to do pairwise comparisons, examining <span class="math inline">\(t_1\)</span> vs <span class="math inline">\(t_2\)</span>, then <span class="math inline">\(t_2\)</span> vs <span class="math inline">\(t_3\)</span>, then <span class="math inline">\(t_1\)</span> vs <span class="math inline">\(t_3\)</span>. Unfortunately, this tends to increase our Type I error!</p>
<p>Think of it this way: if I set my confidence level to 95%, Iâm setting my Type I error rate to <span class="math inline">\(\alpha=0.05\)</span>. In general terms, this means that about 1 out of every 20 times I run my experiment, I would make a type I error. If I went ahead and ran, say, 20 tests comparing two means, my <em>overall</em> Type I error rate is going to increase - thereâs a pretty significant chance that at least one of those comparisons will results in a Type I error!</p>
<p>Instead, we will use a test that allows us to ask: âAre all these means the same?â This is called the <strong>an</strong>alysis <strong>o</strong>f <strong>va</strong>riance, or ANOVA.</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The mean outcome is the same across all groups.</li>
<li><span class="math inline">\(H_A\)</span>: At least one mean differs from the rest.</li>
</ul>
<p>In statistical notation, these hypotheses look like:</p>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2 = \dots = \mu_k\)</span></li>
<li><span class="math inline">\(H_A: \mu_i \ne \mu_j\)</span> for at least one pair <span class="math inline">\((i, j)\)</span></li>
</ul>
<p>where <span class="math inline">\(k\)</span> is the number of means being compared and the notation <span class="math inline">\(\mu_i\)</span> represents the mean for the <span class="math inline">\(i\)</span>th group (<span class="math inline">\(i\)</span> can take on any whole number value between 1 and <span class="math inline">\(k\)</span>).</p>
<p>For ANOVA, we have three key conditions:</p>
<ol style="list-style-type: decimal">
<li>Observations are independent within and across groups.</li>
</ol>
<p>Independence within groups is the way weâve been thinking about independence already. We want to convince ourselves that for any particular group, the observations do not impact each other. For independence across groups, we want to convince ourselves that the groups do not impact each other. Note: if we have a simple random sample, this assumption is always satisfied.</p>
<ol start="2" style="list-style-type: decimal">
<li>Data within each group are approximately normal.</li>
</ol>
<p>If you make a histogram of the data for each group, each histogram will look approximately bell-shaped.</p>
<ol start="3" style="list-style-type: decimal">
<li>Variability is approximately equal across groups.</li>
</ol>
<p>Take the standard deviation for each group and check if they are approximately equal. A boxplot is an appropriate way to do this visually.</p>
<center>
<font size='4'><strong>Why Variance?</strong>
</center>
<p></font></p>
<p>You may have seen the name âanalysis of varianceâ and wondered what the variance has to do with comparing many means. Consider the following boxplots:</p>
<p><img src="IntroStats_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
<p>Is there a difference in the means for Experiment 1? What about Experiment 2?</p>
<p>In fact, the means are <span class="math inline">\(\mu_1 = \mu_4 = 2\)</span>, <span class="math inline">\(\mu_2 = \mu_5 = 1\)</span>, and <span class="math inline">\(\mu_3 = \mu_6 = 0.5\)</span>. But the variances for the Experiment 1 groups are much larger than for the Experiment 2 groups! The larger variances in Experiment 1 obscure any differences between the group means. It is for this reason that we analyze variance as part of our test for differences in means.</p>
<blockquote>
<p>Aside: Why canât we look at the data first and just test the two means that have the largest difference?</p>
<p>When we look at the data <em>and then choose a test</em>, this inflates our Type I error rate! Itâs bad practice and not something we want to engage in as scientists.</p>
</blockquote>
<p>In order to perform an ANOVA, we need to consider whether the sample means differ more than we would expect them to based on natural variation (remember that we expect random samples to produce slightly different sample statistics each time!). This type of variation is called <strong>mean square between groups</strong> or <span class="math inline">\(MSG\)</span>. It has associated degrees of freedom <span class="math inline">\(df_G = k-1\)</span> where <span class="math inline">\(k\)</span> is the number of groups. Note that <span class="math display">\[MSG = \frac{SSG}{df_G}\]</span> where <span class="math inline">\(SSE\)</span> is the <strong>sum of squares group</strong>. If the null hypothesis is true, variation in the sample means is due to chance. In this case, we would expect the MSG to be relatively small.</p>
<p>When I say ârelatively small,â I mean we need to compare this quantity to something. We need some quantity that will give us an idea of how much variability to expect if the null hypothesis is true. This is the <strong>mean square error</strong> or <span class="math inline">\(MSE\)</span>, which has degrees of freedom <span class="math inline">\(df_E = n-k\)</span>. Again, we have the relationship that <span class="math display">\[MSE = \frac{SSE}{df_E}\]</span> where <span class="math inline">\(SSE\)</span> is the <strong>sum of squares error</strong>. These calculations are very similar to the calculation for variance (and standard deviation)! (Note: we will not calculate these quantities by hand, but if you are interested in the mathematical details they are available in the OpenIntro Statistics textbook in the footnote on page 289.)</p>
<p>We compare these two quantities by examining their ratio: <span class="math display">\[F = \frac{MSG}{MSE}\]</span> This is the test statistic for the ANOVA.</p>
<div id="the-f-distribution" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> The F-Distribution</h3>
<p>The <span class="math inline">\(\boldsymbol{F}\)</span><strong>-test</strong> relies on something called the <span class="math inline">\(F\)</span> distribution. The <span class="math inline">\(F\)</span> distribution has two parameters: <span class="math inline">\(df_1=df_G\)</span> and <span class="math inline">\(df_1=df_E\)</span>. The <span class="math inline">\(F\)</span> distribution always takes on positive values, so an <em>extreme</em> or <em>unusual</em> value for the <span class="math inline">\(F\)</span> distribution will correspond to a large (positive) number.</p>
<p>When we run an ANOVA, we almost always use the p-value approach. If you are using <code>R</code> for your distributions, the command is <code>pf(F, df1, df2, lower.tail=FALSE)</code> where <code>F</code> is the test statistic.</p>
<blockquote>
<p><em>Example:</em> Suppose I have a test with 100 observations and 5 groups. I find <span class="math inline">\(MSG = 0.041\)</span> and <span class="math inline">\(MSE = 0.023\)</span>. Then <span class="math display">\[df_G = k-1 = 5-1 = 4\]</span> and <span class="math display">\[df_E = n-k = 100-5 = 95\]</span> The test statistic is <span class="math display">\[f = \frac{0.041}{0.023} = 1.7826\]</span> To find the p-value using <code>R</code>, I would write the command</p>
</blockquote>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="hypothesis-testing-beyond-the-mean.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pf</span>(<span class="fl">1.7826</span>, <span class="dv">4</span>, <span class="dv">95</span>, <span class="at">lower.tail=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.1387132</code></pre>
<blockquote>
<p>and find a p-value of 0.1387.</p>
</blockquote>
<p><a href="https://homepage.divms.uiowa.edu/~mbognar/applets/f.html" target="blank">Here is a nice F-distribution applet</a>. For this applet, <span class="math inline">\(\nu_1 = df_1\)</span> and <span class="math inline">\(\nu_2 = df_2\)</span>. Plug in your <span class="math inline">\(F\)</span> test statistic where it indicates âx =â and your p=value will appear in the red box next to âP(X&gt;x).â When you enter your degrees of freedom, a visualization will appear similar to those in the Rossman and Chance applets we used previously.</p>
<center>
<font size='4'><strong>The ANOVA Table</strong>
</center>
<p></font></p>
<p>Generally, when we run an ANOVA, we create an ANOVA table (or we have software create one for us!). This table looks something like this</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>df</th>
<th>Sum of Squares</th>
<th>Mean Squares</th>
<th>F Value</th>
<th>P-Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>group</td>
<td><span class="math inline">\(df_G\)</span></td>
<td><span class="math inline">\(SSG\)</span></td>
<td><span class="math inline">\(MSG\)</span></td>
<td><span class="math inline">\(F\)</span></td>
<td>p-value</td>
</tr>
<tr class="even">
<td>error</td>
<td><span class="math inline">\(df_E\)</span></td>
<td><span class="math inline">\(SSE\)</span></td>
<td><span class="math inline">\(MSE\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p><em>Example:</em> chick weights</p>
<p><code>R</code> has data on the weights of chicks fed six different feeds (diets). Assume these data are based on a random sample of chicks. There are <span class="math inline">\(n=71\)</span> total observations and <span class="math inline">\(k=6\)</span> different feeds. Letâs assume we want to test with a 0.05 level of significance.</p>
<p>The ANOVA hypotheses are</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: the mean weight is the same for all six feeds.</li>
<li><span class="math inline">\(H_A\)</span>: at least one feed has a mean weight that differs.</li>
</ul>
<p>The summaries for these data are</p>
</blockquote>
<pre><code>##         casein horsebean linseed meatmeal soybean sunflower
## n        12.00     10.00   12.00    11.00   14.00     12.00
## Mean    323.58    160.20  218.75   276.91  246.43    328.92
## Std Dev  64.43     38.63   52.24    64.90   54.13     48.84</code></pre>
<p><img src="IntroStats_files/figure-html/unnamed-chunk-50-1.png" width="672" /><img src="IntroStats_files/figure-html/unnamed-chunk-50-2.png" width="672" /></p>
<blockquote>
<p>The group sizes are relatively small, so itâs difficult to determine how far from normality these data are based on the histograms. We may also run into some issues with constant variance. However, for the sake of the example, letâs push ahead with the ANOVA! Since we usually use software to calculate ANOVAs, Iâve used <code>R</code> to create the following ANOVA table:</p>
</blockquote>
<pre><code>## Analysis of Variance Table
## 
## Response: chickwts$weight
##               Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## chickwts$feed  5 231129   46226  15.365 5.936e-10 ***
## Residuals     65 195556    3009                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<blockquote>
<p>From the table, we can confirm that <span class="math inline">\(df_G = 6-1 = 5\)</span> and <span class="math inline">\(df_E = 71 - 6 = 65\)</span>. The F test statistic is <span class="math display">\[MSG/MSE = 46226 / 3009 = 15.365\]</span> Finally, the p-value is <span class="math inline">\(5.936\times10^{-10}\)</span>. Clearly <span class="math inline">\(5.936\times10^{-10} &lt; \alpha = 0.05\)</span>, so we will reject the null hypothesis and conclude that at least one of the feed groups has a mean weight that differs.</p>
</blockquote>
</div>
<div id="multiple-comparisons-and-type-i-error-rate" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Multiple Comparisons and Type I Error Rate</h3>
<p>Letâs return for a moment to our ANOVA hypotheses:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The mean outcome is the same across all groups.</li>
<li><span class="math inline">\(H_A\)</span>: At least one mean differs from the rest.</li>
</ul>
<p>If we reject <span class="math inline">\(H_0\)</span> and conclude that âat least one mean differs from the rest,â how do we determine which mean(s) differ? <em>If</em> we reject <span class="math inline">\(H_0\)</span>, we will perform a series of two-sample t-tests. But wait! What about the Type I error? Isnât this exactly what we decided we couldnât do when we introduced ANOVA?</p>
<p>In order to avoid this increased Type I error rate, we run these <strong>mulitple comparisons</strong> with a modified significance level. There are several ways to do this, but the most common way is with the <strong>Bonferroni correction</strong>. Here, if we want to test at the <span class="math inline">\(100(1-\alpha)\)</span> level of significance, we run each of our pairwise comparisons with <span class="math display">\[\alpha^* = \alpha/K\]</span> where <span class="math inline">\(K\)</span> is the number of comparisons being considered. For <span class="math inline">\(k\)</span> groups, there are <span class="math display">\[K = \frac{k(k-1)}{2}\]</span> possible pairwise comparisons.</p>
<p>For these comparisons, we use a special pooled estimate of the standard deviation, <span class="math inline">\(s_{\text{pooled}}\)</span> in place of <span class="math inline">\(s_1\)</span> and <span class="math inline">\(s_2\)</span>: <span class="math display">\[\text{standard error} = \sqrt{\frac{s_{\text{pooled}}^2}{n_1} + \frac{s_{\text{pooled}}^2}{n_2}}\]</span> Other than changing <span class="math inline">\(\alpha\)</span> to <span class="math inline">\(\alpha^*\)</span> and the standard error to this new formula, the test is exactly the same as that discussed in the previous section. Note that <span class="math display">\[s_{\text{pooled}} = \sqrt{MSE}\]</span> and the degrees of freedom is <span class="math inline">\(df_E\)</span>.</p>
<blockquote>
<p><em>Example</em>: chick weights</p>
<p>Letâs extend our discussion on the chick weights to multiple comparisons. Since we were able to conclude that at least one feed has a weight that differs, we want to find out where the difference(s) lie!</p>
<p>We will test all possible pairwise comparisons. This will require <span class="math inline">\(K = \frac{6(6-1)}{2} = 15\)</span> tests. The pooled standard deviation is <span class="math inline">\(s_{pooled} = \sqrt{3009} \approx 54.85\)</span>. Letâs walk through the test of casein <span class="math inline">\((\bar{x}_1 = 323.58, n=12)\)</span> vs horsebean <span class="math inline">\((\bar{x}_2 = 160.20, n=10)\)</span>:</p>
<ul>
<li><span class="math inline">\(H_0: \mu_1 = \mu_2\)</span></li>
<li><span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span></li>
</ul>
<p>The estimated difference and standard error are <span class="math display">\[\bar{x}_1 - \bar{x}_2 = 323.58 - 160.20 = 163.38 \quad\quad SE = \sqrt{\frac{54.85^2}{11}+\frac{54.85^2}{9}} = 25.65\]</span> which results in a test statistic of <span class="math inline">\(t=6.37\)</span> and a p-value of <span class="math inline">\(1.11\times10^{-8}\)</span>. We then compare this to <span class="math inline">\(\alpha^* = 0.05/15 = 0.0033\)</span>. Since the p-value of <span class="math inline">\(1.11\times10^{-8} &lt; \alpha^* = 0.0033\)</span>, we reject the null hypothesis and conclude there is a significant difference in mean chick weight between the casein and horsebean feeds.</p>
<p>In order to complete the pairwise comparisons, we would then run the remaining 14 tests. I will leave this as an optional exercise for the particularly motivated student.</p>
</blockquote>
<p>Note: occasionally, we may reject <span class="math inline">\(H_0\)</span> in the ANOVA but may fail to find any statistically significant differences when performing multiple comparisons with the Bonferroni correction. This is ok! It just means we were unable to identify which specific groups differ.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-hypothesis-testing.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["IntroStats.pdf", "IntroStats.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
